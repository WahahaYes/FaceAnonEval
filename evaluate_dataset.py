import argparse

import numpy as np

from src.evaluation.evaluator import Evaluator
from src.evaluation.rank_k_evaluation import (
    rank_k_evaluation,
)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="Evaluate Dataset.",
        description="Evaluates a real dataset (a standard benchmark) against an "
        "anonymized counterpart using a selected evaluation methodology.",
    )

    parser.add_argument(
        "--real_dataset",
        # choices=["CelebA"],
        default="CelebA",
        type=str,
        help="The benchmark dataset to process, which should be placed into the 'Datasets' folder.",
    )
    parser.add_argument(
        "--anonymized_dataset",
        default=None,
        type=str,
        help="(Optional) The anonymized dataset to process, which should have been generated by "
        "'process_dataset.py' and located in the 'Anonymized Datasets' folder.  If this parameter is "
        "not explicitly passed, the dataset will be found based the 'privacy_operation' parameter.",
    )
    parser.add_argument(
        "--privacy_operation",
        choices=["test", "blur_image"],
        default="test",
        type=str,
        help="The privacy operation to compare against.",
    )
    parser.add_argument(
        "--evaluation_method",
        choices=["rank_k"],
        default="rank_k",
        type=str,
        help="The evaluation methodology to use.  Some methods may rely on other arguments as hyperparameters.",
    )
    parser.add_argument(
        "--k",
        default=1,
        type=int,
        help="Choice of k in rank k identity matching.",
    )
    parser.add_argument(
        "--batch_size",
        default=1,
        type=int,
        help="The batch size used when generating embeddings when processing a dataset for the first time.",
    )
    parser.add_argument(
        "--overwrite_embeddings",
        default=False,
        type=bool,
        help="Whether or not to overwrite any existing facial recognition embeddings that "
        "may be cached for datasets that have already been processed before.",
    )
    args = parser.parse_args()

    # Determine dataset paths based on the passed parameters.
    real_dataset_path = f"Datasets//{args.real_dataset}"
    if args.anonymized_dataset is None:
        # TODO: MAKE THIS CONSISTENT WITH WHATERVER NAMING CONVENTION WE DO
        anon_dataset_path = (
            f"Anonymized Datasets//{args.privacy_operation}"
        )
    else:
        anon_dataset_path = f"Anonymized Datasets//{args.anonymized_dataset}"

    # Load in the datasets via Evaluator class
    evaluator = Evaluator(
        real_dataset_path=real_dataset_path,
        anon_dataset_path=anon_dataset_path,
        batch_size=4,
        overwrite_embeddings=args.overwrite_embeddings,
    )

    # Store the hits and misses of the experiment (NOTE: This will probably have to be generalized when we do novel evaluations)
    hits_and_misses: list | None = None
    match args.evaluation_method:
        case "rank_k":
            hits_and_misses = rank_k_evaluation(
                evaluator=evaluator, k=args.k
            )
        case _:
            raise Exception(
                f"Invalid evaluation method argument ({args.evaluation_method})."
            )

    print(f"# of comparisons: {len(hits_and_misses)}")
    print(f"# of hits: {np.sum(hits_and_misses)}")
    print(
        f"# of misses: {len(hits_and_misses) - np.sum(hits_and_misses)}"
    )
    print(f"Average: {np.mean(hits_and_misses):.2%}")
