import argparse

import numpy as np

from src.evaluation.evaluator import Evaluator
from src.evaluation.lfw_validation_evaluation import lfw_validation_evaluation
from src.evaluation.rank_k_evaluation import (
    rank_k_evaluation,
)
from src.parsing import parse_dataset_argument, parse_privacy_mechanism
from src.privacy_mechanisms.privacy_mechanism import PrivacyMechanism

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        prog="Evaluate Dataset.",
        description="Evaluates a real dataset (a standard benchmark) against an "
        "anonymized counterpart using a selected evaluation methodology.",
    )

    parser.add_argument(
        "--dataset",
        choices=["CelebA", "lfw"],
        default="CelebA",
        type=str,
        help="The benchmark dataset to process, which should be placed into the 'Datasets' folder.",
    )
    parser.add_argument(
        "--anonymized_dataset",
        default=None,
        type=str,
        help="(Optional) The anonymized dataset to process, which should have been generated by "
        "'process_dataset.py' and located in the 'Anonymized Datasets' folder.  If this parameter is "
        "not explicitly passed, the dataset will be found based the 'privacy_operation' parameter.",
    )
    parser.add_argument(
        "--privacy_mechanism",
        choices=["test", "blur_image"],
        default="test",
        type=str,
        help="The privacy operation to compare against.",
    )
    # --------------------------------------------------------------------------
    # privacy mechanism-specific arguments
    parser.add_argument(
        "--blur_kernel",
        default=5,
        type=float,
        help="For blurring operations, the size of the blur kernel.",
    )

    # --------------------------------------------------------------------------

    parser.add_argument(
        "--evaluation_method",
        choices=["rank_k", "lfw_validation"],
        default="rank_k",
        type=str,
        help="The evaluation methodology to use.  Some methods may rely on other arguments as hyperparameters.",
    )
    # --------------------------------------------------------------------------
    # evaluation method-specific arguments
    parser.add_argument(
        "--k",
        default=1,
        type=int,
        help="Choice of k in rank k identity matching.",
    )

    # --------------------------------------------------------------------------
    parser.add_argument(
        "--batch_size",
        default=1,
        type=int,
        help="The batch size used when generating embeddings when processing a dataset for the first time.",
    )
    parser.add_argument(
        "--overwrite_embeddings",
        default=False,
        type=bool,
        help="Whether or not to overwrite any existing facial recognition embeddings that "
        "may be cached for datasets that have already been processed before.",
    )
    args = parser.parse_args()

    d_iter, face_dataset, dataset_identity_lookup = parse_dataset_argument(args)

    # Determine dataset paths based on the passed parameters.
    real_dataset_path = f"Datasets//{args.dataset}"
    if args.anonymized_dataset is None:
        p_mech_object: PrivacyMechanism = parse_privacy_mechanism(args)
        anon_dataset_path = (
            f"Anonymized Datasets//{args.dataset}_{p_mech_object.get_suffix()}"
        )
    else:
        anon_dataset_path = f"Anonymized Datasets//{args.anonymized_dataset}"

    # Load in the datasets via Evaluator class
    evaluator = Evaluator(
        real_dataset_path=real_dataset_path,
        anon_dataset_path=anon_dataset_path,
        batch_size=args.batch_size,
        overwrite_embeddings=args.overwrite_embeddings,
    )

    # Store the hits and misses of the experiment (NOTE: This will probably have to be generalized when we do novel evaluations)
    hits_and_misses: list | None = None
    match args.evaluation_method:
        case "rank_k":
            hits_and_misses = rank_k_evaluation(
                evaluator=evaluator, identity_lookup=dataset_identity_lookup, k=args.k
            )
        case "lfw_validation":
            hits_and_misses = lfw_validation_evaluation(
                evaluator=evaluator, identity_lookup=dataset_identity_lookup
            )
        case _:
            raise Exception(
                f"Invalid evaluation method argument ({args.evaluation_method})."
            )

    print(f"# of comparisons: {len(hits_and_misses)}")
    print(f"# of hits: {np.sum(hits_and_misses)}")
    print(f"# of misses: {len(hits_and_misses) - np.sum(hits_and_misses)}")
    print(f"Average: {np.mean(hits_and_misses):.2%}")
